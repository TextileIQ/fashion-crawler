import os
import json
import asyncio
import re
from urllib.parse import unquote
from pathlib import Path
from playwright.async_api import async_playwright
import aiohttp
import aiofiles

# í™˜ê²½ ì„¤ì • (envConfig ëŒ€ì‹ )
class EnvConfig:
    SCROLL_ITERATIONS = 10
    SCROLL_DELAY_MS = 1000
    JSON_INDENT = 2
    IMAGE_LIMIT = 50

env_config = EnvConfig()

REVIEW_IMAGE_SELECTOR = "div.css-s01evr.efs1gt61 img"
REVIEW_TEXT_SELECTOR = "div.css-s01evr.efs1gt61"

async def extract_title_from_zigzag(page, product_page_url):
    """ì§€ê·¸ì¬ê·¸ ìƒí’ˆ í˜ì´ì§€ì—ì„œ ì œëª©ì„ ì¶”ì¶œ"""
    await page.goto(product_page_url, wait_until='networkidle')
    
    title = await page.evaluate('''() => {
        const titleEl = document.querySelector("h1.BODY_15.REGULAR");
        return titleEl ? titleEl.innerText.trim() : null;
    }''')
    
    return title or "ìƒí’ˆëª… ì—†ìŒ"

async def resolve_redirect_and_extract_product_info(page, input_url):
    """ë¦¬ë””ë ‰ì…˜ì„ ì²˜ë¦¬í•˜ê³  ìƒí’ˆ ì •ë³´ë¥¼ ì¶”ì¶œ"""
    final_url = input_url
    
    if "s.zigzag.kr" in input_url or "zigzag.kr/p/" in input_url:
        await page.goto(input_url, wait_until='networkidle')
        final_url = page.url
        print(f"ë¦¬ë””ë ‰ì…˜ ì™„ë£Œ: {final_url}")
        
        # deeplink_url íŒŒë¼ë¯¸í„°ì—ì„œ ì‹¤ì œ URL ì¶”ì¶œ
        deeplink_match = re.search(r'deeplink_url=([^&]+)', final_url)
        if deeplink_match:
            final_url = unquote(deeplink_match.group(1))
    
    # ìƒí’ˆ ID ì¶”ì¶œ
    match = re.search(r'products/(\d+)', final_url)
    if not match:
        raise Exception("ìƒí’ˆ IDë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    product_id = match.group(1)
    review_url = f"https://zigzag.kr/review/list/{product_id}"
    product_page_url = f"https://zigzag.kr/catalog/products/{product_id}"
    
    return {
        'product_id': product_id,
        'review_url': review_url,
        'product_page_url': product_page_url
    }

async def scroll_to_load_all_reviews(page):
    """ëª¨ë“  í›„ê¸°ë¥¼ ë¡œë“œí•˜ê¸° ìœ„í•´ ìŠ¤í¬ë¡¤"""
    for i in range(env_config.SCROLL_ITERATIONS):
        await page.evaluate('() => window.scrollBy(0, window.innerHeight)')
        await asyncio.sleep(env_config.SCROLL_DELAY_MS / 1000)  # msë¥¼ ì´ˆë¡œ ë³€í™˜

async def extract_review_image_urls(page):
    """í›„ê¸° ì´ë¯¸ì§€ URLë“¤ì„ ì¶”ì¶œ"""
    return await page.evaluate(f'''(selector) => {{
        const urls = new Set();
        const images = document.querySelectorAll(selector);
        
        images.forEach((img) => {{
            const src = img.getAttribute("src") || "";
            
            if (src.includes("zigzag.kr/original/review/")) {{
                urls.add(src.split("?")[0]);
            }}
        }});
        
        return Array.from(urls);
    }}''', REVIEW_IMAGE_SELECTOR)

async def save_image_urls_to_json(file_path, urls):
    """ì´ë¯¸ì§€ URLë“¤ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
    # ë””ë ‰í„°ë¦¬ ìƒì„±
    Path(file_path).parent.mkdir(parents=True, exist_ok=True)
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    async with aiofiles.open(file_path, 'w', encoding='utf-8') as f:
        await f.write(json.dumps(urls, indent=env_config.JSON_INDENT, ensure_ascii=False))
    
    print(f"í›„ê¸° ì´ë¯¸ì§€ URL {len(urls)}ê°œ JSON ì €ì¥ ì™„ë£Œ")

async def download_images(product_id, image_urls):
    """ì´ë¯¸ì§€ë“¤ì„ ë‹¤ìš´ë¡œë“œ (ì‹¤ì œ êµ¬í˜„ì€ ë³„ë„ í•¨ìˆ˜ë¡œ)"""
    # ì´ í•¨ìˆ˜ëŠ” ì›ë³¸ì˜ downloadImages í•¨ìˆ˜ë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤
    print(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {len(image_urls)}ê°œ (ìƒí’ˆ ID: {product_id})")
    
    # ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ë¡œì§ì„ ì—¬ê¸°ì— êµ¬í˜„
    download_dir = Path(f"downloads/{product_id}")
    download_dir.mkdir(parents=True, exist_ok=True)
    
    async with aiohttp.ClientSession() as session:
        for i, url in enumerate(image_urls):
            try:
                async with session.get(url) as response:
                    if response.status == 200:
                        filename = f"review_image_{i+1}.jpg"
                        file_path = download_dir / filename
                        
                        async with aiofiles.open(file_path, 'wb') as f:
                            await f.write(await response.read())
                        
                        print(f"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {filename}")
            except Exception as e:
                print(f"ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({url}): {e}")

def guess_category_from_title(title):
    """ì œëª©ì—ì„œ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ì¸¡ (ì‹¤ì œ êµ¬í˜„ì€ ë³„ë„ í•¨ìˆ˜ë¡œ)"""
    # ì›ë³¸ì˜ guessCategoryFromTitle í•¨ìˆ˜ë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤
    # ê°„ë‹¨í•œ ì˜ˆì‹œ êµ¬í˜„
    title_lower = title.lower()
    
    if any(word in title_lower for word in ['ì…”ì¸ ', 'ë¸”ë¼ìš°ìŠ¤', 'shirt']):
        return 'ì˜ë¥˜-ìƒì˜'
    elif any(word in title_lower for word in ['ë°”ì§€', 'íŒ¬ì¸ ', 'pants']):
        return 'ì˜ë¥˜-í•˜ì˜'
    elif any(word in title_lower for word in ['ì‹ ë°œ', 'shoes']):
        return 'ì‹ ë°œ'
    elif any(word in title_lower for word in ['ê°€ë°©', 'bag']):
        return 'ê°€ë°©'
    else:
        return 'ê¸°íƒ€'

async def crawl_zigzag_review_images(product_url, output_path="data/review-images.json"):
    """ì§€ê·¸ì¬ê·¸ í›„ê¸° ì´ë¯¸ì§€ë¥¼ í¬ë¡¤ë§í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        
        try:
            # ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
            product_info = await resolve_redirect_and_extract_product_info(page, product_url)
            product_id = product_info['product_id']
            review_url = product_info['review_url']
            product_page_url = product_info['product_page_url']
            
            # í›„ê¸° í˜ì´ì§€ë¡œ ì´ë™ ë° ìŠ¤í¬ë¡¤
            await page.goto(review_url, wait_until='networkidle')
            await scroll_to_load_all_reviews(page)
            
            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            image_urls = await extract_review_image_urls(page)
            limited = image_urls[:env_config.IMAGE_LIMIT]
            
            if not limited:
                raise Exception("ğŸ¥² í›„ê¸° ì´ë¯¸ì§€ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            
            # JSON ì €ì¥ ë° ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            await save_image_urls_to_json(output_path, limited)
            await download_images(product_id, limited)
            
            print(f"ğŸ–¼ï¸ ì§€ê·¸ì¬ê·¸ í›„ê¸° ì´ë¯¸ì§€ {len(limited)}ì¥ í¬ë¡¤ë§ ì™„ë£Œ ({product_id})")
            
            # ì œëª© ë° ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            title = await extract_title_from_zigzag(page, product_page_url)
            category = guess_category_from_title(title)
            
            return {
                'success': True,
                'product_id': product_id,
                'category': category,
                'imageCount': len(limited),
                'images': limited
            }
            
        except Exception as err:
            print(f"ğŸ¥² ì§€ê·¸ì¬ê·¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {err}")
            raise err
        finally:
            await browser.close()

# ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤

def is_match_site(url, keywords=None):
    """ì‚¬ì´íŠ¸ ë§¤ì¹­ í™•ì¸"""
    if keywords is None:
        keywords = []
    return any(keyword in url for keyword in keywords)

async def send_images_to_server(product_id, source, image_urls, server_api_url):
    """ì„œë²„ë¡œ ì´ë¯¸ì§€ ì •ë³´ ì „ì†¡"""
    try:
        data = {
            "product_id": product_id,
            "source": source,
            "image_urls": image_urls
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(server_api_url, json=data) as response:
                if response.status == 200:
                    print("ì„œë²„ ì „ì†¡ ì„±ê³µ")
                    return await response.json()
                else:
                    raise Exception(f"HTTP {response.status}")
                    
    except Exception as error:
        print(f"ì„œë²„ ì „ì†¡ ì‹¤íŒ¨: {error}")
        raise error
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # ì‹¤ì œ ì§€ê·¸ì¬ê·¸ ìƒí’ˆ URLë¡œ í…ŒìŠ¤íŠ¸
        product_url = "https://zigzag.kr/catalog/products/150012796"  # ì‹¤ì œ ìƒí’ˆ ID ì‚¬ìš©
        result = await crawl_zigzag_review_images(product_url)
        print("í¬ë¡¤ë§ ê²°ê³¼:", result)
    except Exception as e:
        print(f"ì—ëŸ¬ ë°œìƒ: {e}")

if __name__ == "__main__":
    asyncio.run(main())