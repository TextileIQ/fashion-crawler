#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤ (import = ê°€ì ¸ì˜¤ê¸°)
from selenium import webdriver  # ì›¹ ë¸Œë¼ìš°ì €ë¥¼ ìë™ìœ¼ë¡œ ì¡°ì‘í•˜ëŠ” ë„êµ¬
from selenium.webdriver.chrome.service import Service  # í¬ë¡¬ ë¸Œë¼ìš°ì € ì„œë¹„ìŠ¤
from selenium.webdriver.chrome.options import Options  # í¬ë¡¬ ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì •
from selenium.webdriver.common.by import By  # ì›¹í˜ì´ì§€ì—ì„œ ìš”ì†Œë¥¼ ì°¾ëŠ” ë°©ë²•ë“¤
from webdriver_manager.chrome import ChromeDriverManager  # í¬ë¡¬ ë“œë¼ì´ë²„ ìë™ ì„¤ì¹˜
import time  # ì‹œê°„ ê´€ë ¨ ê¸°ëŠ¥ (ëŒ€ê¸°ì‹œê°„ ë“±)
import csv  # CSV íŒŒì¼ ì²˜ë¦¬
import pandas as pd  # ë°ì´í„° ì²˜ë¦¬ ë° ì—‘ì…€ íŒŒì¼ ìƒì„±
from datetime import datetime  # í˜„ì¬ ë‚ ì§œ/ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
import os  # íŒŒì¼ ì‹œìŠ¤í…œ ê´€ë ¨ ê¸°ëŠ¥
import logging  # ë¡œê·¸(ê¸°ë¡) ê´€ë¦¬
import json  # JSON ë°ì´í„° ì²˜ë¦¬
import threading  # ë©€í‹°ìŠ¤ë ˆë”©
from concurrent.futures import ThreadPoolExecutor  # ìŠ¤ë ˆë“œ í’€
import queue  # ìŠ¤ë ˆë“œ ê°„ ë°ì´í„° ì „ë‹¬

def process_single_paper(link_data, thread_id, results_queue, filename):
    """ë‹¨ì¼ ë…¼ë¬¸ ì²˜ë¦¬ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    link, idx = link_data
    
    # ê° ìŠ¤ë ˆë“œë§ˆë‹¤ ë…ë¦½ì ì¸ ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    chrome_options = Options()
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--disable-extensions')
    chrome_options.add_argument('--disable-logging')
    chrome_options.add_argument('--disable-web-security')
    chrome_options.add_argument('--disable-features=VizDisplayCompositor')
    chrome_options.add_argument('--log-level=3')
    chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    service = Service(ChromeDriverManager().install())
    service.log_path = os.devnull
    browser = webdriver.Chrome(service=service, options=chrome_options)
    
    try:
        print(f"ğŸ”„ ìŠ¤ë ˆë“œ {thread_id}: ë…¼ë¬¸ {idx} ì²˜ë¦¬ ì¤‘...")
        browser.get(link)
        time.sleep(2)
        
        # JSON-LD ë°ì´í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ
        json_ld_data = None
        try:
            json_ld_script = browser.find_element(By.XPATH, "//script[@type='application/ld+json']")
            json_ld_data = json.loads(json_ld_script.get_attribute('innerHTML'))
        except:
            pass
        
        # ë…¼ë¬¸ ì œëª©
        title = "ì œëª© ì—†ìŒ"
        if json_ld_data and 'headline' in json_ld_data:
            title = json_ld_data['headline']
        else:
            try:
                title = browser.find_element(By.CLASS_NAME, 'thesis__title').text.strip()
            except:
                pass
        
        # ë…¼ë¬¸ ì €ì
        author = "ì €ì ì—†ìŒ"
        if json_ld_data and 'author' in json_ld_data:
            if isinstance(json_ld_data['author'], list):
                author = ', '.join([author_item.get('name', '') if isinstance(author_item, dict) else str(author_item) for author_item in json_ld_data['author']])
            elif isinstance(json_ld_data['author'], dict):
                author = json_ld_data['author'].get('name', '')
            else:
                author = str(json_ld_data['author'])
        else:
            try:
                author = browser.find_element(By.CLASS_NAME, 'thesis__author').text.strip()
            except:
                pass
        
        # ë…¼ë¬¸ ì´ˆë¡
        abstract = "ì´ˆë¡ ì—†ìŒ"
        try:
            abstract = browser.find_element(By.CLASS_NAME, 'abstractTxt').text.strip()
        except:
            pass
        
        # ë°œí–‰ë…„ë„
        year = "ë…„ë„ ì—†ìŒ"
        if json_ld_data and 'datePublished' in json_ld_data:
            year = json_ld_data['datePublished'][:4]
        else:
            try:
                year_element = browser.find_element(By.CLASS_NAME, 'thesis__year')
                year = year_element.text.strip()
            except:
                pass
        
        # í•™ìˆ ì§€ëª…
        journal = "í•™ìˆ ì§€ ì—†ìŒ"
        if json_ld_data and 'isPartOf' in json_ld_data:
            if isinstance(json_ld_data['isPartOf'], dict):
                journal = json_ld_data['isPartOf'].get('name', '')
            else:
                journal = str(json_ld_data['isPartOf'])
        else:
            try:
                journal = browser.find_element(By.CLASS_NAME, 'thesis__journal').text.strip()
            except:
                pass
        
        # ìˆ˜ë¡ë©´ ì •ë³´
        page_info = "ìˆ˜ë¡ë©´ ì •ë³´ ì—†ìŒ"
        if json_ld_data and 'pagination' in json_ld_data:
            if isinstance(json_ld_data['pagination'], dict):
                page_start = json_ld_data['pagination'].get('pageStart', '')
                page_end = json_ld_data['pagination'].get('pageEnd', '')
                if page_start:
                    page_info = page_start
                    if page_end:
                        page_info += f"-{page_end}"
            else:
                page_info = str(json_ld_data['pagination'])
        else:
            try:
                page_selectors = ['thesis__page', 'thesis__pages', 'page-info', 'thesis__volume', 'thesis__issue']
                for selector in page_selectors:
                    try:
                        page_element = browser.find_element(By.CLASS_NAME, selector)
                        page_info = page_element.text.strip()
                        if page_info:
                            break
                    except:
                        continue
            except:
                pass
        
        # ë°ì´í„° ì •ë¦¬
        paper_info = {
            'ë²ˆí˜¸': idx,
            'ì œëª©': title,
            'ì €ì': author,
            'í•™ìˆ ì§€': journal,
            'ë°œí–‰ë…„ë„': year,
            'ìˆ˜ë¡ë©´': page_info,
            'ì´ˆë¡': abstract,
            'ë§í¬': link,
            'í¬ë¡¤ë§ë‚ ì§œ': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # ê²°ê³¼ë¥¼ íì— ì „ë‹¬
        results_queue.put(paper_info)
        print(f"âœ… ìŠ¤ë ˆë“œ {thread_id}: {title[:30]}... ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ìŠ¤ë ˆë“œ {thread_id}: ë…¼ë¬¸ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì •ë³´ ì €ì¥
        paper_info = {
            'ë²ˆí˜¸': idx,
            'ì œëª©': 'ì²˜ë¦¬ ì‹¤íŒ¨',
            'ì €ì': 'ì²˜ë¦¬ ì‹¤íŒ¨',
            'í•™ìˆ ì§€': 'ì²˜ë¦¬ ì‹¤íŒ¨',
            'ë°œí–‰ë…„ë„': 'ì²˜ë¦¬ ì‹¤íŒ¨',
            'ìˆ˜ë¡ë©´': 'ì²˜ë¦¬ ì‹¤íŒ¨',
            'ì´ˆë¡': 'ì²˜ë¦¬ ì‹¤íŒ¨',
            'ë§í¬': link,
            'í¬ë¡¤ë§ë‚ ì§œ': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        results_queue.put(paper_info)
        
    finally:
        browser.quit()

def crawl_dbpia_papers():
    """DBPIA ë…¼ë¬¸ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
    
    # ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
    logging.getLogger('selenium').setLevel(logging.WARNING)
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    
    # ë¸Œë¼ìš°ì € ì‹œì‘
    print("ğŸŒ ë¸Œë¼ìš°ì €ë¥¼ ì‹œì‘í•˜ëŠ” ì¤‘...")
    
    # Chrome ì˜µì…˜ ì„¤ì •
    chrome_options = Options()
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--disable-extensions')
    chrome_options.add_argument('--disable-logging')
    chrome_options.add_argument('--disable-web-security')
    chrome_options.add_argument('--disable-features=VizDisplayCompositor')
    chrome_options.add_argument('--log-level=3')
    chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    # ë¸Œë¼ìš°ì € ì‹¤í–‰
    service = Service(ChromeDriverManager().install())
    service.log_path = os.devnull
    browser = webdriver.Chrome(service=service, options=chrome_options)
    browser.maximize_window()
    
    try:
        # ê²€ìƒ‰ì–´ ì…ë ¥
        search_word = input("ğŸ” ê²€ìƒ‰í•˜ì‹œê³ ì í•˜ëŠ” ë…¼ë¬¸ ì œëª©ì„ ì…ë ¥í•˜ì„¸ìš”: ")
        
        # URL ìƒì„±
        url = 'https://www.dbpia.co.kr/search/topSearch?startCount=0&collection=ALL&range=A&searchField=ALL&sort=RANK&query={}&srchOption=*&includeAr=false'
        final_url = url.format(search_word)
        
        print(f"ğŸ”— ê²€ìƒ‰ URL: {final_url}")
        browser.get(final_url)
        browser.implicitly_wait(10)
        time.sleep(5)
        
        # í˜ì´ì§€ ë¡œë”© í™•ì¸
        print("ğŸ“„ í˜ì´ì§€ ë¡œë”© ì™„ë£Œ í™•ì¸ ì¤‘...")
        print(f"ğŸ“‹ í˜„ì¬ í˜ì´ì§€ ì œëª©: {browser.title}")
        
        # ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
        try:
            result_elements = browser.find_elements(By.CLASS_NAME, 'thesis__pageLink')
            print(f"ğŸ“Š ì°¾ì€ ë…¼ë¬¸ ë§í¬ ìˆ˜: {len(result_elements)}")
            
            if len(result_elements) == 0:
                print("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")
                page_source = browser.page_source
                if "ê²€ìƒ‰ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤" in page_source or "no results" in page_source.lower():
                    print("âœ… í™•ì¸: ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ë‹¤ëŠ” ë©”ì‹œì§€ ë°œê²¬")
                else:
                    print("ğŸ” í˜ì´ì§€ì— ë‹¤ë¥¸ ìš”ì†Œë“¤ì´ ìˆëŠ”ì§€ í™•ì¸ ì¤‘...")
                    alternative_elements = browser.find_elements(By.CSS_SELECTOR, 'a[href*="thesis"]')
                    print(f"ğŸ”„ ëŒ€ì•ˆ ìš”ì†Œ ìˆ˜: {len(alternative_elements)}")
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ê²°ê³¼ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ë°ì´í„° ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
        paper_data = []
        link_list = []
        processed_titles = set()  # ì¤‘ë³µ ì œëª© ì²´í¬ìš©
        
        # CSV íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'dbpia_papers_{search_word}_{timestamp}.csv'
        
        # ì²« ë²ˆì§¸ í˜ì´ì§€ ë§í¬ ìˆ˜ì§‘
        print("ğŸ“„ ì²« ë²ˆì§¸ í˜ì´ì§€ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
        links = browser.find_elements(By.CLASS_NAME, 'thesis__pageLink')
        print(f"ğŸ”— ì²« í˜ì´ì§€ì—ì„œ ì°¾ì€ ë§í¬ ìˆ˜: {len(links)}")
        
        # ëŒ€ì•ˆ ì„ íƒì ì‹œë„
        if len(links) == 0:
            print("ğŸ¤” ê¸°ë³¸ ì„ íƒìë¡œ ë§í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ë²•ë“¤ì„ ì‹œë„í•´ë³¼ê²Œìš”...")
            alternative_selectors = [
                'a[href*="thesis"]',
                '.thesis a',
                '.search-result a',
                '.paper-item a',
                'a[href*="dbpia"]'
            ]
            
            for selector in alternative_selectors:
                try:
                    alt_links = browser.find_elements(By.CSS_SELECTOR, selector)
                    print(f"ğŸ” ì„ íƒì '{selector}'ë¡œ ì°¾ì€ ë§í¬ ìˆ˜: {len(alt_links)}")
                    if len(alt_links) > 0:
                        links = alt_links
                        break
                except Exception as e:
                    print(f"âŒ ì„ íƒì '{selector}' ì‹œë„ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ë§í¬ ì €ì¥ (ì¤‘ë³µ ì œê±°)
        for link in links:
            href = link.get_attribute('href')
            if href and href not in link_list:
                link_list.append(href)
                print(f"âœ… ë§í¬ ì¶”ê°€: {href[:50]}...")
            elif href in link_list:
                print(f"âš ï¸ ì¤‘ë³µ ë§í¬ ì œì™¸: {href[:50]}...")
        
        # ëª¨ë“  í˜ì´ì§€ ë§í¬ ìˆ˜ì§‘ (ë¬´í•œ ë°˜ë³µ!)
        page_num = 2
        while True:  # ë¬´í•œ ë°˜ë³µ!
            try:
                print(f"ğŸ“„ {page_num}í˜ì´ì§€ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
                xpath = f'//*[@id="pageList"]/a[{page_num}]'
                page_button = browser.find_element(By.XPATH, xpath)
                browser.execute_script("arguments[0].click();", page_button)
                time.sleep(3)
                
                # í˜„ì¬ í˜ì´ì§€ì˜ ë§í¬ ìˆ˜ì§‘
                links = browser.find_elements(By.CLASS_NAME, 'thesis__pageLink')
                new_links_count = 0
                
                for link in links:
                    href = link.get_attribute('href')
                    if href and href not in link_list:
                        link_list.append(href)
                        new_links_count += 1
                
                # ìƒˆë¡œìš´ ë§í¬ê°€ ì—†ìœ¼ë©´ ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ë‹¤ëŠ” ëœ»!
                if new_links_count == 0:
                    print(f"âœ… {page_num}í˜ì´ì§€ì— ìƒˆë¡œìš´ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì™„ë£Œ!")
                    break
                
                print(f"âœ… {page_num}í˜ì´ì§€ì—ì„œ {new_links_count}ê°œ ìƒˆ ë§í¬ ë°œê²¬!")
                page_num += 1  # ë‹¤ìŒ í˜ì´ì§€ë¡œ
                        
            except Exception as e:
                print(f"âŒ {page_num}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                print("ğŸ—¯ï¸ ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì™„ë£Œ!")
                break  # ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ë°˜ë³µ ì¢…ë£Œ
        
        print(f"ğŸ€ ì´ {len(link_list)}ê°œ ë…¼ë¬¸ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ!")
        
        # ë§í¬ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if len(link_list) == 0:
            print("âŒ ìˆ˜ì§‘ëœ ë…¼ë¬¸ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ ì‚¬ì´íŠ¸ êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return None, []
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘! ğŸš€
        print(f"ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘! (ìµœëŒ€ 4ê°œ ìŠ¤ë ˆë“œ)")
        
        # ìŠ¤ë ˆë“œ í’€ ìƒì„± (ìµœëŒ€ 4ê°œ ìŠ¤ë ˆë“œ)
        max_workers = min(4, len(link_list))  # ë…¼ë¬¸ ìˆ˜ê°€ ì ìœ¼ë©´ ìŠ¤ë ˆë“œ ìˆ˜ë„ ì¡°ì •
        results_queue = queue.Queue()
        
        # ë§í¬ ë°ì´í„° ì¤€ë¹„
        link_data_list = [(link, idx) for idx, link in enumerate(link_list, 1)]
        
        # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ëª¨ë“  ì‘ì—… ì œì¶œ
            futures = []
            for i, link_data in enumerate(link_data_list):
                future = executor.submit(process_single_paper, link_data, i+1, results_queue, filename)
                futures.append(future)
            
            # ê²°ê³¼ ìˆ˜ì§‘ (ì‹¤ì‹œê°„)
            completed_count = 0
            while completed_count < len(link_list):
                try:
                    # íì—ì„œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (íƒ€ì„ì•„ì›ƒ 1ì´ˆ)
                    paper_info = results_queue.get(timeout=1)
                    paper_data.append(paper_info)
                    completed_count += 1
                    
                    # ì‹¤ì‹œê°„ CSV ì €ì¥
                    df = pd.DataFrame(paper_data)
                    df.to_csv(filename, index=False, encoding='utf-8-sig')
                    
                    print(f"ğŸ˜º ì§„í–‰ë¥ : {completed_count}/{len(link_list)} ({completed_count/len(link_list)*100:.1f}%)")
                    
                except queue.Empty:
                    # íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ ê³„ì† ëŒ€ê¸°
                    continue
            
            # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
            for future in futures:
                future.result()
        
        # í¬ë¡¤ë§ ì™„ë£Œ ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ‰ === í¬ë¡¤ë§ ì™„ë£Œ! ===")
        print(f"ğŸ“ íŒŒì¼ëª…: {filename}")
        print(f"ğŸ˜º ì´ ë…¼ë¬¸ ìˆ˜: {len(paper_data)}")
        print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {os.path.abspath(filename)}")
        
        # ìˆ˜ì§‘ëœ ë…¼ë¬¸ ëª©ë¡ ë¯¸ë¦¬ë³´ê¸°
        print("\nğŸ“‹ === ìˆ˜ì§‘ëœ ë…¼ë¬¸ ëª©ë¡ ===")
        for i, paper in enumerate(paper_data[:5], 1):
            print(f"{i}. {paper['ì œëª©']}")
            print(f"   ğŸ‘¤ ì €ì: {paper['ì €ì']}")
            print(f"   ğŸ“š í•™ìˆ ì§€: {paper['í•™ìˆ ì§€']}")
            print(f"   ğŸ“„ ìˆ˜ë¡ë©´: {paper['ìˆ˜ë¡ë©´']}")
            print()
        
        if len(paper_data) > 5:
            print(f"... ì™¸ {len(paper_data)-5}ê°œ ë…¼ë¬¸")
        
        return filename, paper_data
        
    except Exception as e:
        error_message = f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
        print(error_message)
        return None, []
        
    finally:
        print("ğŸŒ ë¸Œë¼ìš°ì €ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...")
        browser.quit()

def main():
    print("=" * 50)
    print("ğŸ“ DBPIA ë…¼ë¬¸ í¬ë¡¤ë§ í”„ë¡œê·¸ë¨")
    print("=" * 50)
    
    filename, data = crawl_dbpia_papers()
    
    if filename:
        print(f"\nğŸ‰ âœ… ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ˜ íŒŒì¼ í™•ì¸: {filename}")
    else:
        print("\nâŒ í¬ë¡¤ë§ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()